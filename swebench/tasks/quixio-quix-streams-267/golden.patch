diff --git a/quixstreams/app.py b/quixstreams/app.py
index 28b9843e..bbc3a1b7 100644
--- a/quixstreams/app.py
+++ b/quixstreams/app.py
@@ -1,6 +1,7 @@
 import contextlib
 import logging
 import signal
+from functools import partial
 from typing import Optional, List, Callable, Literal, Mapping
 
 from confluent_kafka import TopicPartition
@@ -30,10 +31,12 @@ from .platforms.quix import (
 )
 from .rowconsumer import RowConsumer
 from .rowproducer import RowProducer
-from .state import StateStoreManager, ChangelogManager
+from .state import StateStoreManager
+from .state.changelog import ChangelogManager, RecoveryManager
 from .state.rocksdb import RocksDBOptionsType
 from .topic_manager import TopicManager, TopicManagerType
 
+
 __all__ = ("Application",)
 
 logger = logging.getLogger(__name__)
@@ -85,7 +88,7 @@ class Application:
         consumer_group: str,
         auto_offset_reset: AutoOffsetReset = "latest",
         auto_commit_enable: bool = True,
-        assignment_strategy: AssignmentStrategy = "range",
+        assignment_strategy: AssignmentStrategy = "cooperative-sticky",
         partitioner: Partitioner = "murmur2",
         consumer_extra_config: Optional[dict] = None,
         producer_extra_config: Optional[dict] = None,
@@ -182,6 +185,8 @@ class Application:
         self._quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None
         self._auto_create_topics = auto_create_topics
         self._topic_validation = topic_validation
+        self._processing = lambda: None
+        self._run_mode = None
 
         if not topic_manager:
             topic_manager = TopicManager()
@@ -203,6 +208,7 @@ class Application:
                     extra_config=producer_extra_config,
                     on_error=on_producer_error,
                 ),
+                consumer=self._consumer,
             )
             if use_changelog_topics
             else None
@@ -223,7 +229,7 @@ class Application:
         consumer_group: str,
         auto_offset_reset: AutoOffsetReset = "latest",
         auto_commit_enable: bool = True,
-        assignment_strategy: AssignmentStrategy = "range",
+        assignment_strategy: AssignmentStrategy = "cooperative-sticky",
         partitioner: Partitioner = "murmur2",
         consumer_extra_config: Optional[dict] = None,
         producer_extra_config: Optional[dict] = None,
@@ -539,6 +545,70 @@ class Application:
                 validation_level=self._topic_validation
             )
 
+    def _process_messages(self, dataframe_composed, start_state_transaction):
+        # Serve producer callbacks
+        self._producer.poll(self._producer_poll_timeout)
+        rows = self._consumer.poll_row(timeout=self._consumer_poll_timeout)
+
+        if rows is None:
+            return
+
+        # Deserializer may return multiple rows for a single message
+        rows = rows if isinstance(rows, list) else [rows]
+        if not rows:
+            return
+
+        first_row = rows[0]
+        topic_name, partition, offset = (
+            first_row.topic,
+            first_row.partition,
+            first_row.offset,
+        )
+        # Create a new contextvars.Context and set the current MessageContext
+        # (it's the same across multiple rows)
+        context = copy_context()
+        context.run(set_message_context, first_row.context)
+
+        with start_state_transaction(
+            topic=topic_name, partition=partition, offset=offset
+        ):
+            for row in rows:
+                try:
+                    # Execute StreamingDataFrame in a context
+                    context.run(dataframe_composed, row.value)
+                except Filtered:
+                    # The message was filtered by StreamingDataFrame
+                    continue
+                except Exception as exc:
+                    # TODO: This callback might be triggered because of Producer
+                    #  errors too because they happen within ".process()"
+                    to_suppress = self._on_processing_error(exc, row, logger)
+                    if not to_suppress:
+                        raise
+
+        # Store the message offset after it's successfully processed
+        self._consumer.store_offsets(
+            offsets=[
+                TopicPartition(
+                    topic=topic_name,
+                    partition=partition,
+                    offset=offset + 1,
+                )
+            ]
+        )
+
+        if self._on_message_processed is not None:
+            self._on_message_processed(topic_name, partition, offset)
+
+    def _recovery(self):
+        try:
+            self._state_manager.do_recovery()
+        except RecoveryManager.RecoveryComplete:
+            self._run_mode = self._processing
+
+    def _do_run_mode(self):
+        return self._run_mode()
+
     def run(
         self,
         dataframe: StreamingDataFrame,
@@ -607,60 +677,13 @@ class Application:
             self._running = True
 
             dataframe_composed = dataframe.compose()
-            while self._running:
-                # Serve producer callbacks
-                self._producer.poll(self._producer_poll_timeout)
-                rows = self._consumer.poll_row(timeout=self._consumer_poll_timeout)
-
-                if rows is None:
-                    continue
-
-                # Deserializer may return multiple rows for a single message
-                rows = rows if isinstance(rows, list) else [rows]
-                if not rows:
-                    continue
-
-                first_row = rows[0]
-                topic_name, partition, offset = (
-                    first_row.topic,
-                    first_row.partition,
-                    first_row.offset,
-                )
-                # Create a new contextvars.Context and set the current MessageContext
-                # (it's the same across multiple rows)
-                context = copy_context()
-                context.run(set_message_context, first_row.context)
-
-                with start_state_transaction(
-                    topic=topic_name, partition=partition, offset=offset
-                ):
-                    for row in rows:
-                        try:
-                            # Execute StreamingDataFrame in a context
-                            context.run(dataframe_composed, row.value)
-                        except Filtered:
-                            # The message was filtered by StreamingDataFrame
-                            continue
-                        except Exception as exc:
-                            # TODO: This callback might be triggered because of Producer
-                            #  errors too because they happen within ".process()"
-                            to_suppress = self._on_processing_error(exc, row, logger)
-                            if not to_suppress:
-                                raise
-
-                # Store the message offset after it's successfully processed
-                self._consumer.store_offsets(
-                    offsets=[
-                        TopicPartition(
-                            topic=topic_name,
-                            partition=partition,
-                            offset=offset + 1,
-                        )
-                    ]
-                )
+            self._processing = partial(
+                self._process_messages, dataframe_composed, start_state_transaction
+            )
+            self._run_mode = self._processing
 
-                if self._on_message_processed is not None:
-                    self._on_message_processed(topic_name, partition, offset)
+            while self._running:
+                self._do_run_mode()
 
             logger.info("Stop processing of StreamingDataFrame")
 
@@ -671,6 +694,8 @@ class Application:
         :param topic_partitions: list of `TopicPartition` from Kafka
         """
         if self._state_manager.stores:
+            if self._state_manager.using_changelogs:
+                self._run_mode = self._recovery
             logger.debug(f"Rebalancing: assigning state store partitions")
             for tp in topic_partitions:
                 # Assign store partitions
@@ -703,6 +728,8 @@ class Application:
         Revoke partitions from consumer and state
         """
         if self._state_manager.stores:
+            if self._state_manager.using_changelogs:
+                self._run_mode = self._recovery
             logger.debug(f"Rebalancing: revoking state store partitions")
             for tp in topic_partitions:
                 self._state_manager.on_partition_revoke(tp)
@@ -712,6 +739,8 @@ class Application:
         Dropping lost partitions from consumer and state
         """
         if self._state_manager.stores:
+            if self._state_manager.using_changelogs:
+                self._run_mode = self._recovery
             logger.debug(f"Rebalancing: dropping lost state store partitions")
             for tp in topic_partitions:
                 self._state_manager.on_partition_lost(tp)
diff --git a/quixstreams/kafka/consumer.py b/quixstreams/kafka/consumer.py
index 04594c31..01df112d 100644
--- a/quixstreams/kafka/consumer.py
+++ b/quixstreams/kafka/consumer.py
@@ -502,6 +502,12 @@ class Consumer:
         """
         return self._consumer.set_sasl_credentials(username, password)
 
+    def incremental_assign(self, partitions: List[TopicPartition]):
+        return self._consumer.incremental_assign(partitions)
+
+    def incremental_unassign(self, partitions: List[TopicPartition]):
+        return self._consumer.incremental_unassign(partitions)
+
     def close(self):
         """
         Close down and terminate the Kafka Consumer.
diff --git a/quixstreams/models/topics.py b/quixstreams/models/topics.py
index 0d5ec596..76fd6851 100644
--- a/quixstreams/models/topics.py
+++ b/quixstreams/models/topics.py
@@ -271,8 +271,18 @@ class Topic:
         )
 
     def deserialize(self, message: ConfluentKafkaMessageProto):
-        # TODO: Implement SerDes for raw messages
-        raise NotImplementedError
+        # TODO: confirm this is a valid context
+        ctx = SerializationContext(topic="", headers=message.headers())
+        return KafkaMessage(
+            key=self._key_deserializer(key, ctx=ctx)
+            if (key := message.key())
+            else None,
+            value=self._value_serializer(value, ctx=ctx)
+            if (value := message.value())
+            else None,
+            headers=message.headers(),
+            timestamp=message.timestamp(),
+        )
 
     def __repr__(self):
         return f'<{self.__class__} name="{self._name}"> '
diff --git a/quixstreams/state/changelog.py b/quixstreams/state/changelog.py
index f43c06f7..ce1b517c 100644
--- a/quixstreams/state/changelog.py
+++ b/quixstreams/state/changelog.py
@@ -1,8 +1,104 @@
-from typing import Optional
+from typing import Optional, Dict, List
 
+from confluent_kafka import TopicPartition as ConfluentPartition
+
+from quixstreams.models import ConfluentKafkaMessageProto
+from quixstreams.kafka import Consumer
 from quixstreams.rowproducer import RowProducer
+from quixstreams.state.types import StorePartition
 from quixstreams.topic_manager import TopicManagerType, BytesTopic
 from quixstreams.types import Headers
+from quixstreams.utils.dicts import dict_values
+
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+class RecoveryPartition:
+    """
+    A changelog partition mapped to a respective StorePartition with helper methods
+    to determine its current recovery status.
+
+    Since `StorePartition`s do recovery directly, it also handles recovery transactions.
+    """
+
+    def __init__(
+        self,
+        topic: str,
+        changelog: str,
+        partition: int,
+        store_partition: StorePartition,
+    ):
+        self.topic = topic
+        self.changelog = changelog
+        self.partition = partition
+        self.store_partition = store_partition
+        self._changelog_lowwater: Optional[int] = None
+        self._changelog_highwater: Optional[int] = None
+
+    class OffsetUpdate(ConfluentKafkaMessageProto):
+        def __init__(self, offset):
+            self._offset = offset
+
+        def offset(self):
+            return self._offset
+
+    @property
+    def offset(self) -> int:
+        return self.store_partition.get_changelog_offset() or 0
+
+    @property
+    def topic_partition(self) -> ConfluentPartition:
+        return ConfluentPartition(self.topic, self.partition)
+
+    @property
+    def changelog_partition(self) -> ConfluentPartition:
+        return ConfluentPartition(self.changelog, self.partition)
+
+    @property
+    def changelog_assignable_partition(self):
+        return ConfluentPartition(self.changelog, self.partition, self.offset)
+
+    @property
+    def needs_recovery(self):
+        has_consumable_offsets = self._changelog_lowwater != self._changelog_highwater
+        state_is_behind = (self._changelog_highwater - self.offset) > 0
+        return has_consumable_offsets and state_is_behind
+
+    @property
+    def needs_offset_update(self):
+        return self._changelog_highwater and (self.offset != self._changelog_highwater)
+
+    def _warn_bad_offset(self):
+        logger.warning(
+            f"The recorded changelog offset in state for "
+            f"{self.changelog}: p{self.partition} was larger than the actual offset "
+            f"available on that topic-partition, likely as a result of some "
+            f"sort of error (mostly likely Kafka or network related). "
+            f"It is possible that the state of any affected message keys may end "
+            f"up inaccurate due to potential double processing. This is an "
+            f"unfortunate possibility with 'at least once' processing guarantees. "
+            f"The offset will now be corrected."
+        )
+
+    def update_offset(self):
+        logger.info(
+            f"topic:partition {self.changelog}:{self.partition} "
+            f"requires an offset update"
+        )
+        if self.offset > self._changelog_highwater:
+            self._warn_bad_offset()
+        self.store_partition.set_changelog_offset(
+            changelog_message=self.OffsetUpdate(self.offset)
+        )
+
+    def recover(self, changelog_message: ConfluentKafkaMessageProto):
+        self.store_partition.recover(changelog_message=changelog_message)
+
+    def set_watermarks(self, lowwater: int, highwater: int):
+        self._changelog_lowwater = lowwater
+        self._changelog_highwater = highwater
 
 
 class ChangelogWriter:
@@ -34,13 +130,22 @@ class ChangelogWriter:
 
 class ChangelogManager:
     """
-    A simple interface for adding changelog topics during store init and
-    generating changelog writers (generally for each new `Store` transaction).
+    A simple interface for managing all things related to changelog topics and is
+    primarily used by the StateStoreManager.
+
+    Facilitates creation of changelog topics and assigning their partitions during
+    rebalances, and handles recovery process loop calls from `Application`.
     """
 
-    def __init__(self, topic_manager: TopicManagerType, producer: RowProducer):
+    def __init__(
+        self,
+        topic_manager: TopicManagerType,
+        consumer: Consumer,
+        producer: RowProducer,
+    ):
         self._topic_manager = topic_manager
         self._producer = producer
+        self._recovery_manager = RecoveryManager(consumer)
 
     def add_changelog(self, source_topic_name: str, suffix: str, consumer_group: str):
         self._topic_manager.changelog_topic(
@@ -49,6 +154,28 @@ class ChangelogManager:
             consumer_group=consumer_group,
         )
 
+    def assign_partition(
+        self,
+        source_topic_name: str,
+        partition: int,
+        store_partitions: Dict[str, StorePartition],
+    ):
+        self._recovery_manager.assign_partitions(
+            source_topic_name=source_topic_name,
+            partition=partition,
+            store_partitions={
+                self._topic_manager.changelog_topics[source_topic_name][
+                    suffix
+                ].name: store_partition
+                for suffix, store_partition in store_partitions.items()
+            },
+        )
+
+    def revoke_partition(self, source_topic_name, partition):
+        self._recovery_manager.revoke_partitions(
+            topic=source_topic_name, partition=partition
+        )
+
     def get_writer(
         self, source_topic_name: str, suffix: str, partition_num: int
     ) -> ChangelogWriter:
@@ -57,3 +184,170 @@ class ChangelogManager:
             partition_num=partition_num,
             producer=self._producer,
         )
+
+    def do_recovery(self):
+        self._recovery_manager.do_recovery()
+
+
+class RecoveryManager:
+    """
+    Manages all aspects of recovery, including managing all topic partition assignments
+    (both source topic and changelogs), generating `RecoveryPartition`s when recovery
+    is required for a given changelog partition, and stopping/revoking said partitions
+    when it determines recovery is complete.
+
+    Important to note that a RecoveryPartition is only generated (and assigned to the
+    consumer) when recovery is necessary, else the partition in question is ignored.
+
+    It will revoke these partitions throughout recovery, which will NOT kick off
+    a rebalance of the consumer since this is done outside the consumer group protocol.
+
+    Recovery is always triggered from any source topic rebalance, which then the
+    `Application` switches its processing loop over to the `RecoveryManager`. The
+    `RecoveryManager` throws the `RecoveryComplete` exception when finished,
+     which is gracefully caught by the `Application`, resuming normal processing.
+    """
+
+    def __init__(self, consumer: Consumer):
+        self._consumer = consumer
+        self._pending_assigns: List[RecoveryPartition] = []
+        self._pending_revokes: List[RecoveryPartition] = []
+        self._partitions: Dict[int, Dict[str, RecoveryPartition]] = {}
+        self._recovery_method = self._recover
+        self._poll_attempts: int = 2
+        self._polls_remaining: int = self._poll_attempts
+
+    class RecoveryComplete(Exception):
+        ...
+
+    @property
+    def in_recovery_mode(self) -> bool:
+        return bool(self._partitions)
+
+    def do_recovery(self):
+        self._recovery_method()
+
+    def assign_partitions(
+        self,
+        source_topic_name: str,
+        partition: int,
+        store_partitions: Dict[str, StorePartition],
+    ):
+        p = None
+        for changelog, store_partition in store_partitions.items():
+            logger.debug(f"Assigning changelog:partition {changelog}:{partition}")
+            p = RecoveryPartition(
+                topic=source_topic_name,
+                changelog=changelog,
+                partition=partition,
+                store_partition=store_partition,
+            )
+            self._pending_assigns.append(p)
+        # Assign manually to immediately pause it (would assign unpaused automatically)
+        # TODO: consider doing all topic (not changelog) assign(s) during the
+        #  Application on_assign call (rather than one at a time here)
+        topic_p = [p.topic_partition]
+        self._consumer.incremental_assign(topic_p)
+        self._consumer.pause(topic_p)
+        self._recovery_method = self._rebalance
+
+    def _partition_cleanup(self, partition: int):
+        if not self._partitions[partition]:
+            del self._partitions[partition]
+
+    def revoke_partitions(self, topic: str, partition: int):
+        # TODO: consider doing all topic (not changelog) unassign(s) during the
+        #  Application on_revoke call (rather than one at a time here)
+        self._consumer.incremental_unassign([ConfluentPartition(topic, partition)])
+        if changelogs := self._partitions.get(partition, {}):
+            for changelog in list(changelogs.keys()):
+                self._pending_revokes.append(changelogs.pop(changelog))
+            self._consumer.pause([p.changelog_partition for p in self._pending_revokes])
+            self._partition_cleanup(partition)
+            self._recovery_method = self._rebalance
+
+    def _handle_pending_assigns(self):
+        assigns = []
+        # TODO: confirm pause needs to be here; if so, refine it to not pause the same
+        #  partition a bunch
+        self._consumer.pause([p.topic_partition for p in self._pending_assigns])
+        while self._pending_assigns:
+            p = self._pending_assigns.pop()
+            p.set_watermarks(
+                *self._consumer.get_watermark_offsets(p.changelog_partition, timeout=10)
+            )
+            if p.needs_recovery:
+                logger.info(
+                    f"topic:partition {p.changelog}:{p.partition} requires recovery"
+                )
+                assigns.append(p)
+                self._partitions.setdefault(p.partition, {})[p.changelog] = p
+            elif p.needs_offset_update:
+                # >0 changelog offset, but none are actually consumable
+                # this is unlikely to happen with At Least Once, but just in case...
+                p.update_offset()
+        if assigns:
+            self._consumer.incremental_assign(
+                [p.changelog_assignable_partition for p in assigns]
+            )
+
+    def _handle_pending_revokes(self):
+        self._consumer.incremental_unassign(
+            [p.changelog_partition for p in self._pending_revokes]
+        )
+        self._pending_revokes = []
+
+    def _rebalance(self):
+        """ """
+        logger.debug("performing a recovery rebalance...")
+        if self._pending_revokes:
+            self._handle_pending_revokes()
+        if self._pending_assigns:
+            self._handle_pending_assigns()
+        self._recovery_method = self._recover
+        self._polls_remaining = self._poll_attempts
+
+    def _update_partition_offsets(self):
+        """
+        update the offsets for assigned partitions, and then revoke them.
+
+        This is a safety measure for when, while recovering, the highwater and
+        changelog consumable offsets don't align: in this case, from failed
+        transactions with Exactly Once processing (stored offset < changelog highwater).
+        """
+        for p in (p_out := dict_values(self._partitions)):
+            if p.needs_offset_update:
+                p.update_offset()
+        self._pending_revokes.extend(p_out)
+        self._partitions = {}
+        self._handle_pending_revokes()
+
+    def _finalize_recovery(self):
+        logger.info("Finalizing recovery and resuming normal processing...")
+        if self._partitions:
+            self._update_partition_offsets()
+        self._consumer.resume(self._consumer.assignment())
+        self._polls_remaining = self._poll_attempts
+        raise self.RecoveryComplete
+
+    def _recover(self):
+        if not self.in_recovery_mode:
+            return self._finalize_recovery()
+
+        if (msg := self._consumer.poll(5)) is None:
+            self._polls_remaining -= 1
+            if not self._polls_remaining:
+                return self._finalize_recovery()
+            return
+
+        changelog = msg.topic()
+        p_num = msg.partition()
+
+        partition = self._partitions[p_num][changelog]
+        partition.recover(changelog_message=msg)
+
+        if not partition.needs_recovery:
+            logger.debug(f"recovery for {msg.topic()}: {msg.partition()} finished!")
+            self._pending_revokes.append(self._partitions[p_num].pop(changelog))
+            self._partition_cleanup(p_num)
+            self._handle_pending_revokes()
diff --git a/quixstreams/state/manager.py b/quixstreams/state/manager.py
index 1461b333..0495e3fa 100644
--- a/quixstreams/state/manager.py
+++ b/quixstreams/state/manager.py
@@ -70,6 +70,13 @@ class StateStoreManager:
         """
         return self._stores
 
+    @property
+    def using_changelogs(self) -> bool:
+        return bool(self._changelog_manager)
+
+    def do_recovery(self):
+        return self._changelog_manager.do_recovery()
+
     def get_store(
         self, topic: str, store_name: str = _DEFAULT_STATE_STORE_NAME
     ) -> Store:
@@ -139,10 +146,16 @@ class StateStoreManager:
         :return: list of assigned `StorePartition`
         """
 
-        store_partitions = []
-        for store in self._stores.get(tp.topic, {}).values():
-            store_partitions.append(store.assign_partition(tp.partition))
-        return store_partitions
+        store_partitions = {}
+        logger.debug(f"Assigning topic:partition {tp.topic}:{tp.partition}")
+        for name, store in self._stores.get(tp.topic, {}).items():
+            store_partition = store.assign_partition(tp.partition)
+            store_partitions[name] = store_partition
+            if self._changelog_manager:
+                self._changelog_manager.assign_partition(
+                    tp.topic, tp.partition, store_partitions
+                )
+        return list(store_partitions.values())
 
     def on_partition_revoke(self, tp: TopicPartition):
         """
@@ -150,8 +163,12 @@ class StateStoreManager:
 
         :param tp: `TopicPartition` from Kafka consumer
         """
-        for store in self._stores.get(tp.topic, {}).values():
-            store.revoke_partition(tp.partition)
+        logger.debug(f"Revoking topic:partition {tp.topic}:{tp.partition}")
+        if stores := self._stores.get(tp.topic, {}).values():
+            if self._changelog_manager:
+                self._changelog_manager.revoke_partition(tp.topic, tp.partition)
+            for store in stores:
+                store.revoke_partition(tp.partition)
 
     def on_partition_lost(self, tp: TopicPartition):
         """
@@ -160,8 +177,7 @@ class StateStoreManager:
 
         :param tp: `TopicPartition` from Kafka consumer
         """
-        for store in self._stores.get(tp.topic, {}).values():
-            store.revoke_partition(tp.partition)
+        self.on_partition_revoke(tp)
 
     def init(self):
         """
diff --git a/quixstreams/state/rocksdb/partition.py b/quixstreams/state/rocksdb/partition.py
index 9a4b7dfb..2563f1d0 100644
--- a/quixstreams/state/rocksdb/partition.py
+++ b/quixstreams/state/rocksdb/partition.py
@@ -7,10 +7,12 @@ from typing import Any, Union, Optional, List, Set, Dict
 from rocksdict import WriteBatch, Rdict, ColumnFamily, AccessType
 from typing_extensions import Self
 
+from quixstreams.models import MessageHeadersTuples, ConfluentKafkaMessageProto
 from quixstreams.state.types import (
     DumpsFunc,
     LoadsFunc,
     PartitionTransaction,
+    PartitionRecoveryTransaction,
     StorePartition,
 )
 from .exceptions import (
@@ -105,6 +107,25 @@ class RocksDBStorePartition(StorePartition):
             changelog_writer=changelog_writer,
         )
 
+    def recover(self, changelog_message: ConfluentKafkaMessageProto):
+        """
+        Completes a stateful recovery from a given changelog message.
+        """
+        with RocksDBPartitionRecoveryTransaction(
+            partition=self,
+            changelog_message=changelog_message,
+        ) as partition:
+            partition.recover()
+
+    def set_changelog_offset(self, offset_only_message: ConfluentKafkaMessageProto):
+        """
+        Set the changelog offset only; usually when the stored offset is "behind" but
+        no messages are left on the changelog.
+        """
+        RocksDBPartitionRecoveryTransaction(
+            partition=self, changelog_message=offset_only_message
+        ).flush()
+
     def write(self, batch: WriteBatch):
         """
         Write `WriteBatch` to RocksDB
@@ -158,7 +179,8 @@ class RocksDBStorePartition(StorePartition):
         offset_bytes = metadata_cf.get(CHANGELOG_OFFSET_KEY)
         if offset_bytes is None:
             offset_bytes = self._db.get(CHANGELOG_OFFSET_KEY)
-        return int_from_int64_bytes(offset_bytes) if offset_bytes is not None else 0
+        if offset_bytes is not None:
+            return int_from_int64_bytes(offset_bytes)
 
     def close(self):
         """
@@ -627,3 +649,101 @@ class RocksDBPartitionTransaction(PartitionTransaction):
     def __exit__(self, exc_type, exc_val, exc_tb):
         if exc_val is None and not self._failed:
             self.maybe_flush()
+
+
+class RocksDBPartitionRecoveryTransaction(PartitionRecoveryTransaction):
+    __slots__ = ("_partition", "_batch", "_failed", "_completed", "_message")
+
+    def __init__(
+        self,
+        partition: RocksDBStorePartition,
+        changelog_message: ConfluentKafkaMessageProto,
+    ):
+        self._partition = partition
+        self._batch = WriteBatch(raw_mode=True)
+        self._message = changelog_message
+        self._failed = False
+        self._completed = False
+
+    class MissingColumnFamilyHeader(Exception):
+        ...
+
+    def _get_header_column_family(self, headers: MessageHeadersTuples) -> ColumnFamily:
+        for t in headers:
+            if t[0] == CHANGELOG_CF_MESSAGE_HEADER:
+                return self._partition.get_column_family_handle(t[1].decode())
+        raise self.MissingColumnFamilyHeader(
+            f"Header '{CHANGELOG_CF_MESSAGE_HEADER}' missing from changelog message!"
+        )
+
+    @_validate_transaction_state
+    def recover(self):
+        """
+        Update the respective db k/v from the changelog message.
+
+        Should only be called once over the lifetime of this object.
+
+        Should flush afterward, which additionally updates the stored offset.
+        """
+        cf_handle = self._get_header_column_family(self._message.headers())
+        key = self._message.key()
+        try:
+            if value := self._message.value():
+                self._batch.put(key, value, cf_handle)
+            else:
+                self._batch.delete(key, cf_handle)
+        except Exception:
+            self._failed = True
+            raise
+
+    @property
+    def completed(self) -> bool:
+        """
+        Check if the transaction is completed.
+
+        It doesn't indicate whether transaction is successful or not.
+        Use `RocksDBTransaction.failed` for that.
+
+        The completed transaction should not be re-used.
+
+        :return: `True` if transaction is completed, `False` otherwise.
+        """
+        return self._completed
+
+    @property
+    def failed(self) -> bool:
+        """
+        Check if the transaction has failed.
+
+        The failed transaction should not be re-used.
+
+        :return: `True` if transaction is failed, `False` otherwise.
+        """
+        return self._failed
+
+    @_validate_transaction_state
+    def flush(self):
+        """
+        Flush the recent updates to the database and empty the update cache.
+        It writes the WriteBatch to RocksDB and marks itself as finished.
+
+        If writing fails, the transaction will be also marked as "failed" and
+        cannot be used anymore.
+        """
+        try:
+            self._batch.put(
+                CHANGELOG_OFFSET_KEY, int_to_int64_bytes(self._message.offset() + 1)
+            )
+            self._partition.write(self._batch)
+        except Exception:
+            self._failed = True
+            raise
+        finally:
+            self._completed = True
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        if exc_val is None and not self._failed:
+            self.flush()
diff --git a/quixstreams/state/types.py b/quixstreams/state/types.py
index 9f227f49..8a42d920 100644
--- a/quixstreams/state/types.py
+++ b/quixstreams/state/types.py
@@ -2,6 +2,8 @@ from typing import Protocol, Any, Optional, Iterator, Callable, Dict, ClassVar
 
 from typing_extensions import Self
 
+from quixstreams.models import ConfluentKafkaMessageProto
+
 DumpsFunc = Callable[[Any], bytes]
 LoadsFunc = Callable[[bytes], Any]
 
@@ -90,9 +92,18 @@ class StorePartition(Protocol):
         State new `PartitionTransaction`
         """
 
+    def recover(self, changelog_message: ConfluentKafkaMessageProto):
+        ...
+
     def get_processed_offset(self) -> Optional[int]:
         ...
 
+    def get_changelog_offset(self) -> Optional[int]:
+        ...
+
+    def set_changelog_offset(self, changelog_message: ConfluentKafkaMessageProto):
+        ...
+
 
 class State(Protocol):
     """
@@ -184,3 +195,43 @@ class PartitionTransaction(State):
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         ...
+
+
+class PartitionRecoveryTransaction(Protocol):
+    """
+    A class for managing recovery for a StorePartition from a changelog message
+    """
+
+    @property
+    def failed(self) -> bool:
+        """
+        Return `True` if transaction failed to update data at some point.
+
+        Failed transactions cannot be re-used.
+        :return: bool
+        """
+
+    @property
+    def completed(self) -> bool:
+        """
+        Return `True` if transaction is completed.
+
+        Completed transactions cannot be re-used.
+        :return: bool
+        """
+        ...
+
+    def recover(self):
+        ...
+
+    def flush(self):
+        """
+        Flush the recent updates and last processed offset to the storage.
+        """
+        ...
+
+    def __enter__(self):
+        ...
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        ...
diff --git a/quixstreams/topic_manager.py b/quixstreams/topic_manager.py
index ca2cc9e0..7c8b41b3 100644
--- a/quixstreams/topic_manager.py
+++ b/quixstreams/topic_manager.py
@@ -5,6 +5,7 @@ from abc import abstractmethod
 from typing import Dict, List, Mapping, Optional, Set, Literal, Protocol, ClassVar
 
 from quixstreams.platforms.quix import QuixKafkaConfigsBuilder
+from quixstreams.utils.dicts import dict_values
 from .kafka.admin import Admin
 from .models.serializers import DeserializerType, SerializerType
 from .models.topics import Topic, TopicConfig, TopicList, TopicMap
@@ -26,26 +27,6 @@ class BytesTopic(Topic):
         )
 
 
-def dict_values(d: object) -> List:
-    """
-    Recursively unpacks a set of nested dicts to get a flattened list of leaves,
-    where "leaves" are the first non-dict item.
-
-    i.e {"a": {"b": {"c": 1}, "d": 2}, "e": 3} becomes [1, 2, 3]
-
-    :param d: initially, a dict (with potentially nested dicts)
-
-    :return: a list with all the leaves of the various contained dicts
-    """
-    if d:
-        if isinstance(d, dict):
-            return [i for v in d.values() for i in dict_values(v)]
-        elif isinstance(d, list):
-            return d
-        return [d]
-    return []
-
-
 def affirm_ready_for_create(topics: TopicList):
     """
     Validate a list of topics is ready for creation attempt
@@ -92,6 +73,11 @@ class TopicManagerType(Protocol):
 
     @property
     def changelog_topics(self) -> Dict[str, Dict[str, BytesTopic]]:
+        """
+        Changelogs stored as {source_topic_name: {suffix: Topic}}
+
+        returns:
+        """
         return self._changelog_topics
 
     @property
diff --git a/quixstreams/utils/dicts.py b/quixstreams/utils/dicts.py
new file mode 100644
index 00000000..94996673
--- /dev/null
+++ b/quixstreams/utils/dicts.py
@@ -0,0 +1,21 @@
+from typing import List
+
+
+def dict_values(d: object) -> List:
+    """
+    Recursively unpacks a set of nested dicts to get a flattened list of leaves,
+    where "leaves" are the first non-dict item.
+
+    i.e {"a": {"b": {"c": 1}, "d": 2}, "e": 3} becomes [1, 2, 3]
+
+    :param d: initially, a dict (with potentially nested dicts)
+
+    :return: a list with all the leaves of the various contained dicts
+    """
+    if d:
+        if isinstance(d, dict):
+            return [i for v in d.values() for i in dict_values(v)]
+        elif isinstance(d, list):
+            return d
+        return [d]
+    return []
