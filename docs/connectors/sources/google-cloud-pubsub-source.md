# Google Cloud (GCP) Pub/Sub Source

!!! info

    This is a **Community** connector. Test it before using in production.

    To learn more about differences between Core and Community connectors, see the [Community and Core Connectors](../community-and-core.md) page.

This source enables reading from a Google Cloud Pub/Sub topic, dumping it to a
kafka topic using desired `StreamingDataFrame`-based transformations.

## How to use Google Cloud Pub/Sub Source

To use Pub/Sub Source, hand `PubSubSource` to `app.dataframe()`.

For the full description of expected parameters of each, see the [PubSub Source API](../../api-reference/sources.md#pubsubsource) page.  

```python
from quixstreams import Application
from quixstreams.sources.community.pubsub import PubSubSource
from os import environ

source = PubSubSource(
    # Suggested: pass JSON-formatted credentials from an environment variable.
    service_account_json = environ["PUBSUB_SERVICE_ACCOUNT_JSON"],
    project_id="<project ID>",
    topic_id="<topic ID>",  # NOTE: NOT the full /x/y/z path!
    subscription_id="<subscription ID>",  # NOTE: NOT the full /x/y/z path!
    create_subscription=True,
)
app = Application(
    broker_address="localhost:9092",
    auto_offset_reset="earliest",
    consumer_group="gcp",
    loglevel="INFO"
)
sdf = app.dataframe(source=source).print(metadata=True)

if __name__ == "__main__":
    app.run()
```

## Testing locally

Rather than connect to Google Cloud, you can alternatively test your application using 
a local "emulated" Pub/Sub host via docker:

1. DO NOT pass a `service_account_json` to `PubSubSource`, instead set environment variable:
    
    `PUBSUB_EMULATOR_HOST=localhost:8085`

2. execute in terminal:

    `docker run -d --name pubsub-emulator -p 8085:8085 gcr.io/google.com/cloudsdktool/google-cloud-cli:emulators gcloud beta emulators pubsub start --host-port=0.0.0.0:8085`

## Message keys

If a Pub/Sub message was published as an ordered message, it will contain a 
message key, else an empty string.

**Pub/Sub Message keys will be used as the Kafka message key
regardless of the Pub/Sub's `enable_message_ordering` subscription setting**.

The [message read order](#message-read-ordering) depends on the subscription setting.

## Message read ordering

Message read order depends on 

1. a Pub/Sub message being published as an ordered message (has a key)
2. a Pub/Sub subscription enabling ordered messages (set at subscription creation)

Pub/Sub Source **cannot change any existing subscription settings**, including ones 
generated by itself!

Assuming proper permissions, new subscriptions can be generated using `create_subscriptions=True`, 
with ordering enabled via `enable_message_ordering=True` (they must be set **simultaneously**
 else ordering won't work, due to above).

## Message data format/schema

Incoming message keys will be strings (non-ordered messages will be empty strings).

Incoming message values will be in bytes, so transform accordingly in your SDF directly.

## Source Processing Guarantees

The Pub/Sub Source offers "at-least-once" guarantees: there is no confirmation that
message acknowledgements for the Pub/Sub Subscriber succeeded.

As such, in rare circumstances where acknowledgement ends up failing, messages may be 
processed (produced) more than once (and additionally, out of their original order).
    
## Topic

The default topic name the Application dumps to is `gcp-pubsub_{subscription_name}_{topic_name}`.
