import logging
from pathlib import Path
from time import sleep
from typing import Optional, Union

from quixstreams.models import Topic, TopicConfig
from quixstreams.sources import Source

from .compressions import CompressionName
from .formats import FORMATS, Format, FormatName
from .origins import LocalOrigin
from .origins.base import Origin

__all__ = ("FileSource",)

logger = logging.getLogger(__name__)


class FileSource(Source):
    """
    Ingest a set of files from a desired origin into Kafka by iterating through the
    provided folder and processing all nested files within it.

    Origins include a local filestore, AWS S3, or Microsoft Azure.

    FileSource defaults to a local filestore (LocalOrigin) + JSON format.

    Expects folder and file structures as generated by the related FileSink connector:

    ```
    my_topics/
    ├── topic_a/
    │   ├── 0/
    │   │   ├── 0000.ext
    │   │   └── 0011.ext
    │   └── 1/
    │       ├── 0003.ext
    │       └── 0016.ext
    └── topic_b/
        └── etc...
    ```

    Intended to be used with a single topic (ex: topic_a), but will recursively read
    from whatever entrypoint is passed to it.

    File format structure depends on the file format.

    See the `.formats` and `.compressions` modules to see what is supported.

    Example Usage:

    ```python
    from quixstreams import Application
    from quixstreams.sources.community.file import FileSource
    from quixstreams.sources.community.file.origins import S3Origin

    app = Application(broker_address="localhost:9092", auto_offset_reset="earliest")

    origin = S3Origin(
        bucket="<YOUR BUCKET>",
        aws_access_key_id="<YOUR KEY ID>",
        aws_secret_access_key="<YOUR SECRET KEY>",
        aws_region="<YOUR REGION>",
    )
    source = FileSource(
        directory="path/to/your/topic_folder/",
        origin=origin,
        format="json",
        compression="gzip",
    )
    sdf = app.dataframe(source=source).print(metadata=True)
    # YOUR LOGIC HERE!

    if __name__ == "__main__":
        app.run()
    ```
    """

    def __init__(
        self,
        directory: Union[str, Path],
        format: Union[Format, FormatName] = "json",
        origin: Origin = LocalOrigin(),
        compression: Optional[CompressionName] = None,
        replay_speed: float = 1.0,
        name: Optional[str] = None,
        shutdown_timeout: float = 10,
    ):
        """
        :param directory: a directory to recursively read through; it is recommended to
            provide the path to a given topic folder (ex: `/path/to/topic_a`).
        :param format: what format the message files are in (ex: json, parquet).
            Optionally, can provide a `Format` instance if more than compression
            is necessary to define (compression will then be ignored).
        :param origin: an Origin type (defaults to reading local files).
        :param compression: what compression is used on the given files, if any.
        :param replay_speed: Produce the messages with this speed multiplier, which
            roughly reflects the time "delay" between the original message producing.
            Use any float >= 0, where 0 is no delay, and 1 is the original speed.
            NOTE: Time delay will only be accurate per partition, NOT overall.
        :param name: The name of the Source application (Default: last folder name).
        :param shutdown_timeout: Time in seconds the application waits for the source
            to gracefully shutdown
        """
        if not replay_speed >= 0:
            raise ValueError("`replay_speed` must be a positive value")

        self._directory = Path(directory)
        self._origin = origin
        self._formatter = _get_formatter(format, compression)
        self._replay_speed = replay_speed
        self._previous_timestamp = None
        self._previous_partition = None
        super().__init__(
            name=name or self._directory.name, shutdown_timeout=shutdown_timeout
        )

    def _replay_delay(self, current_timestamp: int):
        """
        Apply the replay speed by calculating the delay between messages
        based on their timestamps.
        """
        if self._previous_timestamp is not None:
            time_diff_seconds = (current_timestamp - self._previous_timestamp) / 1000
            replay_diff_seconds = time_diff_seconds * self._replay_speed
            if replay_diff_seconds > 0.01:  # only sleep when diff is "big enough"
                logger.debug(f"Sleeping for {replay_diff_seconds} seconds...")
                sleep(replay_diff_seconds)
        self._previous_timestamp = current_timestamp

    def _check_file_partition_number(self, file: Path):
        """
        Checks whether the next file is the start of a new partition so the timestamp
        tracker can be reset.
        """
        partition = int(file.parent.name)
        if self._previous_partition != partition:
            self._previous_timestamp = None
            self._previous_partition = partition
            logger.debug(f"Beginning reading partition {partition}")

    def _produce(self, record: dict):
        kafka_msg = self._producer_topic.serialize(
            key=record["_key"],
            value=record["_value"],
            timestamp_ms=record["_timestamp"],
        )
        self.produce(
            key=kafka_msg.key, value=kafka_msg.value, timestamp=kafka_msg.timestamp
        )

    def default_topic(self) -> Topic:
        """
        Uses the file structure to generate the desired partition count for the
        internal topic.
        :return: the original default topic, with updated partition count
        """
        topic = super().default_topic()
        topic.config = TopicConfig(
            num_partitions=self._origin.get_folder_count(self._directory) or 1,
            replication_factor=1,
        )
        return topic

    def run(self):
        while self._running:
            logger.info(f"Reading files from topic {self._directory.name}")
            for file in self._origin.file_collector(self._directory):
                logger.debug(f"Reading file {file}")
                self._check_file_partition_number(file)
                filestream = self._origin.get_raw_file_stream(file)
                for record in self._formatter.read(filestream):
                    if timestamp := record.get("_timestamp"):
                        self._replay_delay(timestamp)
                    self._produce(record)
                self.flush()
            return


def _get_formatter(
    formatter: Union[Format, FormatName], compression: Optional[CompressionName]
) -> Format:
    if isinstance(formatter, Format):
        return formatter
    elif format_obj := FORMATS.get(formatter):
        return format_obj(compression=compression)

    allowed_formats = ", ".join(FormatName.__args__)
    raise ValueError(
        f'Invalid format name "{formatter}". '
        f"Allowed values: {allowed_formats}, "
        f"or an instance of a subclass of `Format`."
    )
