import logging
import queue
import threading
from pathlib import Path
from time import sleep
from typing import BinaryIO, Optional, Union

from typing_extensions import Self

from quixstreams.models import Topic, TopicConfig
from quixstreams.sources import Source

from .compressions import CompressionName
from .formats import FORMATS, Format, FormatName
from .origins import LocalOrigin
from .origins.base import Origin

__all__ = ("FileSource",)

logger = logging.getLogger(__name__)


class StopThread(Exception): ...


class FileFetcher:
    """
    Serves individual files while downloading another in the background.
    """

    def __init__(self, origin: Origin, filepath: Path):
        self._origin = origin
        self._filepath = filepath
        self._queue = queue.Queue(maxsize=1)
        self._download_thread = threading.Thread(target=self._download_files)
        self._stop_event = threading.Event()
        self._thread_exception = StopIteration
        self._download_thread.start()

    def __iter__(self) -> Self:
        return self

    def __next__(self) -> tuple[Path, BinaryIO]:
        # Blocking queue to account for various download times
        file_content = self._queue.get(block=True)
        if file_content is None:
            self.stop()
            raise self._thread_exception
        return file_content

    def stop(self):
        if not self._stop_event.is_set():
            logger.debug("Stopping download thread...")
            self._stop_event.set()
            self._download_thread.join(timeout=10)

    def _add_file_to_queue(self, filepath: Path):
        """
        Add downloaded file to queue with soft blocking (to check for stop_events).
        """
        filestream = self._origin.get_raw_file_stream(filepath)
        while not self._stop_event.is_set():
            try:
                self._queue.put((filepath, filestream), block=True, timeout=3)
                logger.debug(f"{filepath} added to queue...")
                return
            except queue.Full:
                pass
        raise StopThread()

    def _download_files(self):
        try:
            for file in self._origin.file_collector(self._filepath):
                if self._stop_event.is_set():
                    return
                self._add_file_to_queue(file)
        except Exception as e:
            if not isinstance(e, StopThread):
                logger.error("Download thread encountered an error", exc_info=e)
                self._thread_exception = e
        finally:
            self._queue.put(None, block=True)


class FileSource(Source):
    """
    Ingest a set of files from a desired origin into Kafka by iterating through the
    provided folder and processing all nested files within it.

    Origins include a local filestore, AWS S3, or Microsoft Azure.

    FileSource defaults to a local filestore (LocalOrigin) + JSON format.

    Expects folder and file structures as generated by the related FileSink connector:

    ```
    my_topics/
    ├── topic_a/
    │   ├── 0/
    │   │   ├── 0000.ext
    │   │   └── 0011.ext
    │   └── 1/
    │       ├── 0003.ext
    │       └── 0016.ext
    └── topic_b/
        └── etc...
    ```

    Intended to be used with a single topic (ex: topic_a), but will recursively read
    from whatever entrypoint is passed to it.

    File format structure depends on the file format.

    See the `.formats` and `.compressions` modules to see what is supported.

    Example Usage:

    ```python
    from quixstreams import Application
    from quixstreams.sources.community.file import FileSource
    from quixstreams.sources.community.file.origins import S3Origin

    app = Application(broker_address="localhost:9092", auto_offset_reset="earliest")

    origin = S3Origin(
        bucket="<YOUR BUCKET>",
        aws_access_key_id="<YOUR KEY ID>",
        aws_secret_access_key="<YOUR SECRET KEY>",
        aws_region="<YOUR REGION>",
    )
    source = FileSource(
        directory="path/to/your/topic_folder/",
        origin=origin,
        format="json",
        compression="gzip",
    )
    sdf = app.dataframe(source=source).print(metadata=True)
    # YOUR LOGIC HERE!

    if __name__ == "__main__":
        app.run()
    ```
    """

    def __init__(
        self,
        directory: Union[str, Path],
        format: Union[Format, FormatName] = "json",
        origin: Origin = LocalOrigin(),
        compression: Optional[CompressionName] = None,
        replay_speed: float = 1.0,
        name: Optional[str] = None,
        shutdown_timeout: float = 10,
    ):
        """
        :param directory: a directory to recursively read through; it is recommended to
            provide the path to a given topic folder (ex: `/path/to/topic_a`).
        :param format: what format the message files are in (ex: json, parquet).
            Optionally, can provide a `Format` instance if more than compression
            is necessary to define (compression will then be ignored).
        :param origin: an Origin type (defaults to reading local files).
        :param compression: what compression is used on the given files, if any.
        :param replay_speed: Produce the messages with this speed multiplier, which
            roughly reflects the time "delay" between the original message producing.
            Use any float >= 0, where 0 is no delay, and 1 is the original speed.
            NOTE: Time delay will only be accurate per partition, NOT overall.
        :param name: The name of the Source application (Default: last folder name).
        :param shutdown_timeout: Time in seconds the application waits for the source
            to gracefully shutdown
        """
        if not replay_speed >= 0:
            raise ValueError("`replay_speed` must be a positive value")

        self._directory = Path(directory)
        self._origin = origin
        self._formatter = _get_formatter(format, compression)
        self._replay_speed = replay_speed
        self._previous_timestamp = None
        self._previous_partition = None
        self._file_fetcher: Optional[FileFetcher] = None
        super().__init__(
            name=name or self._directory.name, shutdown_timeout=shutdown_timeout
        )

    def _replay_delay(self, current_timestamp: int):
        """
        Apply the replay speed by calculating the delay between messages
        based on their timestamps.
        """
        if self._previous_timestamp is not None:
            time_diff_seconds = (current_timestamp - self._previous_timestamp) / 1000
            replay_diff_seconds = time_diff_seconds * self._replay_speed
            if replay_diff_seconds > 0.01:  # only sleep when diff is "big enough"
                logger.debug(f"Sleeping for {replay_diff_seconds} seconds...")
                sleep(replay_diff_seconds)
        self._previous_timestamp = current_timestamp

    def _check_file_partition_number(self, file: Path):
        """
        Checks whether the next file is the start of a new partition so the timestamp
        tracker can be reset.
        """
        partition = int(file.parent.name)
        if self._previous_partition != partition:
            self._previous_timestamp = None
            self._previous_partition = partition
            logger.debug(f"Beginning reading partition {partition}")

    def _produce(self, record: dict):
        kafka_msg = self._producer_topic.serialize(
            key=record["_key"],
            value=record["_value"],
            timestamp_ms=record["_timestamp"],
        )
        self.produce(
            key=kafka_msg.key, value=kafka_msg.value, timestamp=kafka_msg.timestamp
        )

    def default_topic(self) -> Topic:
        """
        Uses the file structure to generate the desired partition count for the
        internal topic.
        :return: the original default topic, with updated partition count
        """
        topic = super().default_topic()
        topic.config = TopicConfig(
            num_partitions=self._origin.get_folder_count(self._directory) or 1,
            replication_factor=1,
        )
        return topic

    def stop(self):
        if self._file_fetcher:
            self._file_fetcher.stop()
        super().stop()

    def run(self):
        self._file_fetcher = FileFetcher(self._origin, self._directory)
        logger.info(f"Reading files from topic {self._directory.name}")
        for file_name, content in self._file_fetcher:
            logger.debug(f"Reading file {file_name}")
            self._check_file_partition_number(file_name)
            for record in self._formatter.read(content):
                if timestamp := record.get("_timestamp"):
                    self._replay_delay(timestamp)
                self._produce(record)
            self.flush()


def _get_formatter(
    formatter: Union[Format, FormatName], compression: Optional[CompressionName]
) -> Format:
    if isinstance(formatter, Format):
        return formatter
    elif format_obj := FORMATS.get(formatter):
        return format_obj(compression=compression)

    allowed_formats = ", ".join(FormatName.__args__)
    raise ValueError(
        f'Invalid format name "{formatter}". '
        f"Allowed values: {allowed_formats}, "
        f"or an instance of a subclass of `Format`."
    )
