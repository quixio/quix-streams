import logging
from concurrent.futures import Future, ThreadPoolExecutor
from pathlib import Path
from time import sleep
from typing import BinaryIO, Optional, Union

from typing_extensions import Self

from quixstreams.models import Topic, TopicConfig
from quixstreams.sources import (
    ClientConnectFailureCallback,
    ClientConnectSuccessCallback,
    Source,
)

from .compressions import CompressionName
from .formats import FORMATS, Format, FormatName
from .origins import LocalOrigin
from .origins.base import Origin

__all__ = ("FileSource",)

logger = logging.getLogger(__name__)


class FileFetcher:
    """
    Serves individual files while downloading another in the background.
    """

    def __init__(self, origin: Origin, filepath: Path):
        self._origin = origin
        self._file_names = iter(self._origin.file_collector(filepath))
        self._stopped: bool = False
        self._executor = ThreadPoolExecutor(max_workers=1)
        self._downloading_file_name: Optional[Path] = None
        self._downloading_file_content: Optional[Future] = None
        self._download_next_file()

    def __iter__(self) -> Self:
        return self

    def __next__(self) -> tuple[Path, BinaryIO]:
        if self._stopped:
            raise StopIteration

        try:
            file_name = self._downloading_file_name
            file_content = self._downloading_file_content.result()
            self._download_next_file()
            return file_name, file_content
        except Exception as e:
            logger.error("FileFetcher encountered an error", exc_info=e)
            self.stop()
            raise e

    def stop(self):
        logger.info("Stopping file download thread...")
        self._stopped = True
        self._executor.shutdown(wait=True, cancel_futures=True)

    def _download_next_file(self):
        try:
            self._downloading_file_name = next(self._file_names)
            logger.debug(f"Beginning download of {self._downloading_file_name}...")
            self._downloading_file_content = self._executor.submit(
                self._origin.get_raw_file_stream, self._downloading_file_name
            )
        except StopIteration:
            logger.info("No further files to download.")
            self.stop()


class FileSource(Source):
    """
    Ingest a set of files from a desired origin into Kafka by iterating through the
    provided folder and processing all nested files within it.

    Origins include a local filestore, AWS S3, or Microsoft Azure.

    FileSource defaults to a local filestore (LocalOrigin) + JSON format.

    Expects folder and file structures as generated by the related FileSink connector:

    ```
    my_topics/
    ├── topic_a/
    │   ├── 0/
    │   │   ├── 0000.ext
    │   │   └── 0011.ext
    │   └── 1/
    │       ├── 0003.ext
    │       └── 0016.ext
    └── topic_b/
        └── etc...
    ```

    Intended to be used with a single topic (ex: topic_a), but will recursively read
    from whatever entrypoint is passed to it.

    File format structure depends on the file format.

    See the `.formats` and `.compressions` modules to see what is supported.

    Example Usage:

    ```python
    from quixstreams import Application
    from quixstreams.sources.community.file import FileSource
    from quixstreams.sources.community.file.origins import S3Origin

    app = Application(broker_address="localhost:9092", auto_offset_reset="earliest")

    origin = S3Origin(
        bucket="<YOUR BUCKET>",
        aws_access_key_id="<YOUR KEY ID>",
        aws_secret_access_key="<YOUR SECRET KEY>",
        aws_region="<YOUR REGION>",
    )
    source = FileSource(
        directory="path/to/your/topic_folder/",
        origin=origin,
        format="json",
        compression="gzip",
    )
    sdf = app.dataframe(source=source).print(metadata=True)
    # YOUR LOGIC HERE!

    if __name__ == "__main__":
        app.run()
    ```
    """

    def __init__(
        self,
        directory: Union[str, Path],
        format: Union[Format, FormatName] = "json",
        origin: Origin = LocalOrigin(),
        compression: Optional[CompressionName] = None,
        replay_speed: float = 1.0,
        name: Optional[str] = None,
        shutdown_timeout: float = 30,
        on_client_connect_success: Optional[ClientConnectSuccessCallback] = None,
        on_client_connect_failure: Optional[ClientConnectFailureCallback] = None,
    ):
        """
        :param directory: a directory to recursively read through; it is recommended to
            provide the path to a given topic folder (ex: `/path/to/topic_a`).
        :param format: what format the message files are in (ex: json, parquet).
            Optionally, can provide a `Format` instance if more than compression
            is necessary to define (compression will then be ignored).
        :param origin: an Origin type (defaults to reading local files).
        :param compression: what compression is used on the given files, if any.
        :param replay_speed: Produce the messages with this speed multiplier, which
            roughly reflects the time "delay" between the original message producing.
            Use any float >= 0, where 0 is no delay, and 1 is the original speed.
            NOTE: Time delay will only be accurate per partition, NOT overall.
        :param name: The name of the Source application (Default: last folder name).
        :param shutdown_timeout: Time in seconds the application waits for the source
            to gracefully shutdown
        :param on_client_connect_success: An optional callback made after successful
            client authentication, primarily for additional logging.
        :param on_client_connect_failure: An optional callback made after failed
            client authentication (which should raise an Exception).
            Callback should accept the raised Exception as an argument.
            Callback must resolve (or propagate/re-raise) the Exception.
        """
        self._directory = Path(directory)
        super().__init__(
            name=name or self._directory.name,
            shutdown_timeout=shutdown_timeout,
            on_client_connect_success=on_client_connect_success,
            on_client_connect_failure=on_client_connect_failure,
        )

        if not replay_speed >= 0:
            raise ValueError("`replay_speed` must be a positive value")

        self._origin = origin
        self._formatter = _get_formatter(format, compression)
        self._replay_speed = replay_speed
        self._previous_timestamp = None
        self._previous_partition = None
        self._file_fetcher: Optional[FileFetcher] = None

    def _replay_delay(self, current_timestamp: int):
        """
        Apply the replay speed by calculating the delay between messages
        based on their timestamps.
        """
        if self._previous_timestamp is not None:
            time_diff_seconds = (current_timestamp - self._previous_timestamp) / 1000
            replay_diff_seconds = time_diff_seconds * self._replay_speed
            if replay_diff_seconds > 0.01:  # only sleep when diff is "big enough"
                logger.debug(f"Sleeping for {replay_diff_seconds} seconds...")
                sleep(replay_diff_seconds)
        self._previous_timestamp = current_timestamp

    def _check_file_partition_number(self, file: Path):
        """
        Checks whether the next file is the start of a new partition so the timestamp
        tracker can be reset.
        """
        partition = int(file.parent.name)
        if self._previous_partition != partition:
            self._previous_timestamp = None
            self._previous_partition = partition
            logger.debug(f"Beginning reading partition {partition}")

    def _produce(self, record: dict):
        kafka_msg = self._producer_topic.serialize(
            key=record["_key"],
            value=record["_value"],
            timestamp_ms=record["_timestamp"],
        )
        self.produce(
            key=kafka_msg.key, value=kafka_msg.value, timestamp=kafka_msg.timestamp
        )

    def default_topic(self) -> Topic:
        """
        Uses the file structure to generate the desired partition count for the
        internal topic.
        :return: the original default topic, with updated partition count
        """
        topic = super().default_topic()
        topic.create_config = TopicConfig(
            num_partitions=self._origin.get_folder_count(self._directory) or 1,
            replication_factor=1,
        )
        return topic

    def stop(self):
        if self._file_fetcher:
            self._file_fetcher.stop()
        super().stop()

    def setup(self):
        self._origin = self._origin.__enter__()

    def run(self):
        with self._origin:  # for conveniently exiting the context
            self._file_fetcher = FileFetcher(self._origin, self._directory)
            logger.info(f"Reading files from topic {self._directory.name}")
            for file_name, content in self._file_fetcher:
                logger.debug(f"Reading file {file_name}")
                self._check_file_partition_number(file_name)
                for record in self._formatter.read(content):
                    if timestamp := record.get("_timestamp"):
                        self._replay_delay(timestamp)
                    self._produce(record)
                self.flush()


def _get_formatter(
    formatter: Union[Format, FormatName], compression: Optional[CompressionName]
) -> Format:
    if isinstance(formatter, Format):
        return formatter
    elif format_obj := FORMATS.get(formatter):
        return format_obj(compression=compression)

    allowed_formats = ", ".join(FormatName.__args__)
    raise ValueError(
        f'Invalid format name "{formatter}". '
        f"Allowed values: {allowed_formats}, "
        f"or an instance of a subclass of `Format`."
    )
